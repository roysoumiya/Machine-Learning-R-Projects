train_ind<-sample(seq_len(nrow(energy_data)),size = energy_train)
train_ind
train_data = energy_data[train_ind,]
train_data
test_data = energy_data[-train_ind,]
test_data
premodel = rpart(Y2~ X1+X2+X3+X4+X5+X6+X7+X8, data = train_data, method="anova")
premodel
premodel1 = rpart(Y1~ X1+X2+X3+X4+X5+X6+X7+X8, data = train_data, method="anova")
premodel1
p1= predict(premodel,test_data,type = "vector")
p= predict(premodel1,test_data,type = "vector")
plot(p1,type='h',col="green",ylim=c(10,max(p1,p)))
lines(p,type='h',lty=2,col="red")
?rpart
?rpart.plot
?rpart
?prp
library(XLConnect)
energy_data = readWorksheetFromFile("Desktop/Energy_Efficiency.xlsx",sheet = 1)
energy_data
energy_train<-floor(0.8 * nrow(energy_data))
energy_train
set.seed(13)
train_ind<-sample(seq_len(nrow(energy_data)),size = energy_train)
train_ind
train_data = energy_data[train_ind,]
train_data
test_data = energy_data[-train_ind,]
test_data
modelY2 = rpart(Y2~ X1+X2+X3+X4+X5+X6+X7+X8, data = train_data, method="anova")
model
modelY2
model1Y1 = rpart(Y1~ X1+X2+X3+X4+X5+X6+X7+X8, data = train_data, method="anova")
modelY1
modelY1 = rpart(Y1~ X1+X2+X3+X4+X5+X6+X7+X8, data = train_data, method="anova")
modelY1
p1= predict(modelY2,test_data,type = "vector")
p= predict(modelY1,test_data,type = "vector")
plot(p1,type='h',col="green",ylim=c(10,max(p1,p)))
lines(p,type='h',lty=2,col="red")
library(XLConnect)
titanic_train=read.csv("Desktop/Titanic_train.csv")
titanic_train
titanic_test=read.csv("Desktop/Titanic_test.csv")
titanic_test
library(rpart)
library(rpart.plot)
tree_model=rpart(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked, data=titanic_train, method="class")
tree_model
prp(tree_model)
rpart.plot(tree_model)
prp(tree_model)
rpart.plot(tree_model)
plot(p1,type='h',col="green",ylim=c(10,max(p1,p)))
c
lines(p,type='h',lty=2,col="red")
library(dendextend)
install.packages("dendextend")
install.packages("corrplot")
library(dendextend)
library(corrplot)
animals <- cluster::animals
colnames(animals) <- c("warm-blooded",
"can fly",
"vertebrate",
"endangered",
"live in groups",
"have hair")
dend_r <- animals %>% dist(method = "man") %>% hclust(method = "ward.D") %>% as.dendrogram %>% ladderize %>%
color_branches(k=4)
dend_c <- t(animals) %>% dist(method = "man") %>% hclust(method = "com") %>% as.dendrogram %>% ladderize%>%
color_branches(k=3)
some_col_func <- function(n) (colorspace::diverge_hcl(n, h = c(246, 40), c = 96, l = c(65, 90)))
gplots::heatmap.2(as.matrix(animals-1),
main = "Attributes of Animals",
srtCol = 35,
Rowv = animals,
Colv = t(animals),
trace="row", hline = NA, tracecol = "darkgrey",
margins =c(6,3),
key.xlab = "no / yes",
denscol = "grey",
density.info = "density",
col = some_col_func
)
animals <- cluster::animals
colnames(animals) <- c("warm-blooded",
"can fly",
"vertebrate",
"endangered",
"live in groups",
"have hair")
dend_r <- animals %>% dist(method = "man") %>% hclust(method = "ward.D") %>% as.dendrogram %>% ladderize %>%
color_branches(k=4)
dend_c <- t(animals) %>% dist(method = "man") %>% hclust(method = "com") %>% as.dendrogram %>% ladderize%>%
color_branches(k=3)
some_col_func <- function(n) rev(colorspace::heat_hcl(n, c = c(80, 30), l = c(30, 90), power = c(1/5, 1.5)))
some_col_func <- colorspace::diverge_hcl
some_col_func <- colorspace::sequential_hcl
some_col_func <- function(n) (colorspace::diverge_hcl(n, h = c(246, 40), c = 96, l = c(65, 90)))
par(mar = c(3,3,3,3))
library(gplots)
gplots::heatmap.2(as.matrix(animals-1),
main = "Attributes of Animals",
srtCol = 35,
Rowv = animals,
Colv = t(animals),
trace="row", hline = NA, tracecol = "darkgrey",
margins =c(6,3),
key.xlab = "no / yes",
denscol = "grey",
density.info = "density",
col = some_col_func
)
library(ggplots)
install.packages("gplots")
library(gplots)
gplots::heatmap.2(as.matrix(animals-1),
main = "Attributes of Animals",
srtCol = 35,
Rowv = animals,
Colv = t(animals),
trace="row", hline = NA, tracecol = "darkgrey",
margins =c(6,3),
key.xlab = "no / yes",
denscol = "grey",
density.info = "density",
col = some_col_func
)
hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty",
"median", "centroid", "ward.D2")
animals_dendlist <- dendlist()
hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty",
"median", "centroid", "ward.D2")
animals_dendlist <- dendlist()
for(i in seq_along(hclust_methods)) {
tmp_dend <-  animals %>% dist(method = "man") %>%
hclust(method = hclust_methods[i]) %>% as.dendrogram
animals_dendlist <- dendlist(animals_dendlist, tmp_dend)
}
names(animals_dendlist) <- hclust_methods
votes.repub_dendlist
cophenetic_cors <- cor.dendlist(animals_dendlist)
corrplot::corrplot(cophenetic_cors, "pie", "lower")
remove_median <- dendlist(animals_dendlist, which = c(1:8)[-6] )
FM_cors <- cor.dendlist(remove_median, method = "FM_index", k = 4)
corrplot::corrplot(FM_cors, "pie", "lower")
install.packages("ggplot2")
library(ggplot2)
install.packages("ggplot2")
library(ggplot2)
clusterPlot <- function(type) {
clusters <- hclust(dist(iris[, 3:4]), method = type)
plot(clusters)
clusterCut <- cutree(clusters, 3)
show(table(clusterCut, iris$Species)) # show required, else will not print
ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$Species)) +
geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) +
scale_color_manual(values = c('black', 'red', 'green'))
}
clusterPlot('complete')
clusterPlot('average')
clusterPlot('single')
clusterPlot('ward.D')
data(mtcars)
mydata <- mtcars
mydata <- na.omit(mydata) # listwise deletion of missing
mydata <- scale(mydata) # standardize variables
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
fit <- kmeans(mydata, 5) # 5 cluster solution
aggregate(mydata,by=list(fit$cluster),FUN=mean)
mydata <- data.frame(mydata, fit$cluster)
d <- dist(mydata, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward")
fit <- kmeans(mydata, 5) # 5 cluster solution
aggregate(mydata,by=list(fit$cluster),FUN=mean)
mydata <- data.frame(mydata, fit$cluster)
d <- dist(mydata, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward")
fit <- hclust(d, method="ward.D")
plot(fit) # display dendogram
groups <- cutree(fit, k=5) # cut tree into 5 clusters
rect.hclust(fit, k=5, border="red")
library(datasets)
head(iris)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
set.seed(20)
irisCluster <- kmeans(iris[, 3:4], 3, nstart = 20)
irisCluster
table(irisCluster$cluster, iris$Species)
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()
mydata <- iris
mydata$Species <- NULL
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:10) wss[i] <- sum(kmeans(mydata, centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
for (i in 2:10) wss[i] <- sum(kmeans(mydata, centers=i)$betweenss)
plot(1:10, wss, type="b", xlab="Number of Clusters", ylab="Between groups sum of squares")
library(ggplot2)
library(XLConnect)
install.packages("XLConnect")
install.packages("XLConnect")
play <- readWorksheetFromFile("C:/MY FILES/Data/Play/play.xlsx", sheet = 1)
submit_data <- data.frame(Name = titanic_test$Name, Survived = Prediction_data)
install.packages("ggplot2")
install.packages("XLConnect")
library(ggplot2)
library(XLConnect)
play <- readWorksheetFromFile("Desktop/play.xlsx", sheet = 1)
play
clusters <- hclust(dist(play[, 2:3]))
plot(clusters)
clusterCut <- cutree(clusters, 2)
table(clusterCut, play$Decision)
clusters <- hclust(dist(play[, 2:3]), method = 'average')
plot(clusters)
clusterCut <- cutree(clusters, 2)
table(clusterCut, play$Decision)
ggplot(play, aes(play$Temperature, play$Humidity, color = play$Decision)) +
geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) +
scale_color_manual(values = c('black', 'red', 'green'))
install.packages("ggplot2")
library(ggplot2)
clusterPlot <- function(type) {
clusters <- hclust(dist(iris[, 3:4]), method = type)
plot(clusters)
clusterCut <- cutree(clusters, 3)
show(table(clusterCut, iris$Species)) # show required, else will not print
ggplot(iris, aes(Petal.Length, Petal.Width, color = iris$Species)) +
geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) +
scale_color_manual(values = c('black', 'red', 'green'))
}
clusterPlot('complete')
clusterPlot('average')
clusterPlot('single')
clusterPlot('complete')
clusterPlot('average')
clusterPlot('single')
install.packages("glmnet")
install.packages("dplyr")
library(dplyr)
library(glmnet)
?c
#If FALSE, install package XLConnect:
install.packages("XLConnectJars")
install.packages("XLConnect")
install.packages("rJava")
install.packages("glmnet")
install.packages("dplyr")
#or (used this)
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
library(XLConnectJars)
library(XLConnect)
library(rJava)
library(dplyr)
library(glmnet)
x = data.frame()
install.packages("dplyr")
install.packages("glmnet")
install.packages("glmnet")
for(i in seq(60,300,4))
{
x<- rbind(x,(i*pi/180))
}
head(x)
tail(x)
nrow(x)
n <- seq(0,0.15,length.out = 61)
n
c.norm <- rnorm(n)
y <- sin(x)+c.norm # Gaussian/normal error
y
data<- cbind(x,y)
data
colnames(data)<- c("X", "Y")
data
plt<-plot(data)
X_2<- data$X^2
X_3<- data$X^3
X_4<- data$X^4
X_5<- data$X^5
X_6<- data$X^6
X_7<- data$X^7
X_8<- data$X^8
X_9<- data$X^9
X_10<- data$X^10
X_11<- data$X^11
X_12<- data$X^12
X_13<- data$X^13
X_14<- data$X^14
X_15<- data$X^15
datax<- cbind(data,X_2,X_3,X_4,X_5,X_6,X_7,X_8,X_9,X_10,X_11,X_12,X_13,X_14,X_15)
head(datax)
fit<-lm(Y ~ X ,data = datax)
coeff=coefficients(fit)
print(fit)
summary(fit)
coef(summary(fit))
print(fit)
summary(fit)
y_pred<-fitted(fit)
y_pred
coeff
rss<-sum((y_pred - data$Y)^2)
m1<-cbind(rss,coeff)
m1
fit2<-lm(Y ~ X,X_2 ,data = datax)
coeff2=coefficients(fit2)
y_pred2<-fitted(fit2)
rss2<-sum((y_pred2 - data$Y)^2)
m2<-cbind(rss2,rss,coeff,coeff2)
m2
fit3<-lm(Y ~ X,X_2,X_3 ,data = datax)
coeff3=coefficients(fit6)
y_pred3<-fitted(fit6)
rss3<-sum((y_pred3 - data$Y)^2)
m3<-cbind(rss2,rss,coeff,coeff2,rss3,coeff3)
m3
fitx<-lm(Y ~. ,data = datax)
coeffx=coefficients(fitx)
y_predx<-fitted(fitx)
rssx<-sum((y_predx - data$Y)^2)
mx<-cbind(rss2,rss,coeff,coeff2,rss3,coeff3,rssx,coeffx)
mx
?glmnet
x <- datax[,-2]
y<-datax[,2]
y
ridge.mod <- glmnet(as.matrix(x), y, alpha = 0)
any(grepl("XLConnect",
installed.packages()))
install.packages("XLConnectJars")
install.packages("XLConnect")
install.packages("rJava")
install.packages("rpart")
install.packages("rpart.plot")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
library(XLConnectJars)
library(XLConnect)
library(rJava)
library(XLConnect)
titanic_train=read.csv("Desktop/Titanic_train.csv")
titanic_train
titanic_test=read.csv("Desktop/Titanic_test.csv")
install.packages("rpart")
titanic_train=read.csv("Desktop/Titanic_train.csv")
titanic_train
titanic_test=read.csv("Desktop/Titanic_test.csv")
titanic_test
?rpart
?rpart.plot
?prp
library(rpart)
library(rpart.plot)
tree_model=rpart(Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked,
data=titanic_train, method="class")
tree_model
prp(tree_model)
rpart.plot(tree_model)
Prediction_data <- predict(tree_model, titanic_test, type = "class")
submit_data <- data.frame(Name = titanic_test$Name, Survived = Prediction_data)
Prediction_data <- predict(tree_model, titanic_test, type = "class")
submit_data <- data.frame(Name = titanic_test$Name, Survived = Prediction_data)
submit_data
writeWorksheetToFile(file = "Desktop/Titanic_predicted.xlsx", data = submit_data, sheet = "Sheet1")
titanic_test=read.csv("Desktop/Titanic_test.csv")
titanic_test
submit_data <- data.frame(PassengerId = titanic_test$PassengerId, Survived = Prediction_data)
submit_data
writeWorksheetToFile(file = "Desktop/Titanic_predicted.xlsx", data = submit_data, sheet = "Sheet1")
writeWorksheetToFile(file = "Desktop/Titanic_predicted.csv", data = submit_data, sheet = "Sheet1")
submit_data <- data.frame(PassengerId = titanic_test$PassengerId, Survived = Prediction_data)
submit_data
writeWorksheetToFile(file = "Desktop/Titanic_predicted.csv", data = submit_data, sheet = "Sheet1")
write.csv(submit_data, file = "Titanic_predicted.csv")
submit_data <- data.frame(PassengerId = titanic_test$PassengerId, Survived = Prediction_data)
submit_data
write.csv(submit_data, file = "Desktop/Titanic_predicted.csv")
read.table(file = 'Desktop/train.tsv', sep = '\t', header = TRUE)
read.table(file = 'Desktop/train.tsv', sep = '\t', header = TRUE)
x <- readLines('Desktop/train.tsv')
sapply(x, length)
library(caret)
install.packages("caret")
library(caret)
library(e1071)
data(GermanCredit)
dataset = GermanCredit
dataset
summary(dataset)
str(dataset)
dataset[,1:7] = as.data.frame(lapply(dataset[,1:7], scale))
str(dataset)
?lapply
?scale
sample_index = sample(1000, 200)
test_dateset = dataset[sample_index,]
train_dateset = dataset[-sample_index,]
test_dateset
train_dateset
radialobj <- tune(svm, Class~., kernel = "radial", data = train_dateset, ranges = list(gamma=2^(-15:3) , cost=2^(-5:15)))
radialobj <- svm(Class~., kernel = "radial", data = train_dateset, ranges = list(gamma=2^(-15:3) , cost=2^(-5:15)))
install.packages("e1071")
radialobj <- tune(svm, Class~., kernel = "radial", data = train_dateset, ranges = list(gamma=2^(-15:3) , cost=2^(-5:15)))
library(e1071)
radialobj <- tune(svm, Class~., kernel = "radial", data = train_dateset, ranges = list(gamma=2^(-15:3) , cost=2^(-5:15)))
radialobj <- tune(svm, Class~., kernel = "radial", data = train_dateset, ranges = list(gamma=2^(-15:3) , cost=2^(-5:15)))
radialobj
radialobj <- svm(Class~., kernel = "radial", data = train_dateset, ranges = list(gamma=2^(-15:3) , cost=2^(-5:15)))
radialobj
predictions1 <-  predict(radialobj, test_dateset[-10])
table(test_dateset[,10], predictions1)
radialobj <- svm(Class~., kernel = "radial", data = train_dateset, ranges = list(gamma=2^(-15:3) , cost=2^(-5:15)))
summary(radialobj)
radialobj <- svm(Class~., kernel = "radial", data = train_dateset, ranges = list(gamma=2^(-15:3) , cost=2^(-5:15)), scale = F)
summary(radialobj)
radialmodel <- svm(Class~., kernel = "radial", data = train_dateset, ranges = list(gamma=2^(-15:3) , cost=2^(-5:15)), scale = F)
summary(radialmodel)
linearmodel <- svm(Class~., kernel = "linear", data = train_dateset, ranges = list(gamma=2^(-15:3), cost=2^(-5:15)), scale = F)
summary(linearmodel)
polymodel <- svm(Class~.,kernel = "polynomial", data = train_dateset, ranges = list(gamma=2^(-15:3), cost=2^(-5:15)), scale = F)
summary(polymodel)
sigmoidmodel <- svm(Class~.,kernel = "sigmoid", data = train_dateset, ranges = list(gamma=2^(-15:3), cost=2^(-5:15)), scale = F)
summary(sigmoidmodel)
predictions1 <-  predict(radialmodel, test_dateset[-10])
predictions2 <-  predict(linearmodel, test_dateset[-10])
predictions3 <-  predict(polymodel, test_dateset[-10])
predictions4 <-  predict(sigmoidmodel, test_dateset[-10])
predictions1
table(test_dateset[,10], predictions1)
table(test_dateset[,10], predictions2)
table(test_dateset[,10], predictions3)
table(test_dateset[,10], predictions4)
library("e1071")
head(iris,5)
attach(iris)
x <- subset(iris, select=-Species)
y <- Species
svm_model <- svm(Species ~ ., data=iris, cost=100)
summary(svm_model)
varImp(svm_model, scale=FALSE)
plot(svm_model, iris,
Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
pred <- predict(svm_model, head(iris), decision.values = TRUE)
pred
svm_model1 <- svm(x,y)
summary(svm_model1)
pred <- predict(svm_model1,x)
system.time(pred <- predict(svm_model1,x))
table(pred,y)
svm_tune <- tune(svm, train.x=x, train.y=y,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
print(svm_tune)
svm_model_after_tune <- svm(Species ~ ., data=iris, kernel="radial", cost=100, gamma=0.5)
summary(svm_model_after_tune)
pred <- predict(svm_model_after_tune,x)
system.time(predict(svm_model_after_tune,x))
table(pred,y)
pred <- predict(svm_model_after_tune,x)
system.time(predict(svm_model_after_tune,x))
table(pred,y)
table(pred,y)
rm(list=ls(all=TRUE))
x1s <- c(.5,1,1,2,3,3.5,     1,3.5,4,5,5.5,6)
x2s <- c(3.5,1,2.5,2,1,1.2,  5.8,3,4,5,4,1)
ys <- c(rep(+1,6),          rep(-1,6))
my.data <- data.frame(x1=x1s, x2=x2s, type=as.factor(ys))
my.data
library('e1071')
svm.model <- svm(type ~ ., data=my.data,
type='C-classification', kernel='linear', scale=FALSE, cost = 1) # cost = 1
svm.model
svm.model <- svm(type ~ ., data=my.data,
type='C-classification', kernel='linear', scale=FALSE, cost = 0.5)
svm.model
plot(my.data[,-3],col=(ys+3)/2, pch=19, xlim=c(-1,6), ylim=c(-1,6))
points(my.data[svm.model$index,c(1,2)],col="blue",cex=2)
x1min = min(x1s); x1max = max(x1s);
x2min = min(x2s); x2max = max(x2s);
coef1 = sum(svm.model$coefs*x1s[svm.model$index]);
coef2 = sum(svm.model$coefs*x2s[svm.model$index]);
lines(c(x1min,x1max),  (svm.model$rho-coef1*c(x1min, x1max))/coef2)
lines(c(x1min,x1max),  (svm.model$rho+1-coef1*c(x1min, x1max))/coef2, lty=2)
lines(c(x1min,x1max),  (svm.model$rho-1-coef1*c(x1min, x1max))/coef2, lty=2)
library(e1071)
library(MASS)
library(ggplot2)
dataset <- read.csv('C:/MY FILES/data/Breast Cancer/wdbc.data', head = FALSE)
index <- 1:nrow(dataset)
testindex <- sample(index, trunc(length(index)*30/100))
library(e1071)
library(MASS)
library(ggplot2)
dataset <- read.csv('Desktop/wdbc.data.txt', head = FALSE)
index <- 1:nrow(dataset)
testindex <- sample(index, trunc(length(index)*30/100))
testset <- dataset[testindex,]
trainset <- dataset[-testindex,]
names(dataset)
tuned <- tune.svm(V2~., data = trainset, gamma = 10^(-6:-1), cost = 10^(-1:1))
summary(tuned)
model  <- svm(V2~., data = trainset, kernel = "radial", gamma = 0.001, cost = 10)
summary(model)
prediction <- predict(model, testset[,-2])
tab <- table(pred = prediction, true = testset[,2])
tab
