{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red52\green52\blue52;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c26667\c26667\c26667;\cssrgb\c100000\c100000\c100000;}
\margl1440\margr1440\vieww14380\viewh10140\viewkind0
\deftab720
\pard\pardeftab720\sl240\sa200\qc\partightenfactor0

\f0\b\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
ASSIGNMENT 3
\b0\fs20 \
\pard\pardeftab720\sl240\sa200\partightenfactor0
\cf2 \

\fs24 1. Read pruning, entropy, and information gain in decision tree. Compute entropy manually from slide, put your steps and result in word file.\
\
2. In the last class, I covered C4.5 decision tree classification technique where the dependent variable is categorical and splitting is based on information gain. \
Please read about decision tree regression where the dependent variable is continuous and splitting is based on standard deviation reduction: \
{\field{\*\fldinst{HYPERLINK "http://www.saedsayad.com/decision_tree_reg.htm"}}{\fldrslt http://www.saedsayad.com/decision_tree_reg.htm}}. \
You will now have assignments on both. \
Implement:\
(i) Decision tree classification with Titanic dataset (predict Survived) \
(ii) Decision tree regression Energy efficiency Dataset (outcome y1 or y2) in R. \
You can use \'91rpart\'92\'a0library but there are other options.\
You can find dataset information from links below:\cb1 \uc0\u8232 {\field{\*\fldinst{HYPERLINK "https://www.kaggle.com/c/titanic"}}{\fldrslt \cb3 https://www.kaggle.com/c/titanic}}\uc0\u8232 {\field{\*\fldinst{HYPERLINK "https://archive.ics.uci.edu/ml/datasets/Energy+efficiency"}}{\fldrslt \cb3 https://archive.ics.uci.edu/ml/datasets/Energy+efficiency}}\cb3 \
\
3. Create report including your exploration about the data, data preprocessing, your models and the evaluation of your models.\'a0
\fs20 \
}