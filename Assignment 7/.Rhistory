with(sent_agg, hist(ave_sentiment))
mean(reviews_all$stars)
mean(sent_agg$ave_sentiment) #Measured on a scale from -4 to 4 (where 0 is neutral)
#'Highlight' function in Sentimentr package helps us obtain the reviews with highest sentiment and lowest sentiment
best_reviews <- slice(reviews_all, top_n(sent_agg, 3, ave_sentiment)$element_id)
with(best_reviews, sentiment_by(comments)) %>% highlight() #Result is displayed in HTML
worst_reviews <- slice(reviews_all, top_n(sent_agg, 3, -ave_sentiment)$element_id)
with(worst_reviews, sentiment_by(comments)) %>% highlight() #Result is displayed in HTML
install.packages("tidyr")
install.packages("tidyr")
install.packages("purrr")
library(tidyverse)
install.packages("purrr")
library(tidytext)
word_tb <- reviews_all %>%
unnest_tokens(word, TXT)
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative")) %>%
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative")) %>%
reviews_all %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words = 100))
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative")) %>%
reviews_all %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words = 100))
library(tidyverse)
library(tidytext)
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative")) %>%
reviews_all %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words = 100))
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative")) %>%
best_reviews %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words = 100))
prod_code = "B0043WCH66" #ASIN Code of a product
url <- paste0("https://www.amazon.com/dp/", prod_code) #URL of Amazon's product
doc <- read_html(url)
prod <- html_nodes(doc, "#productTitle") %>% html_text() %>% gsub("\n", "", .) %>% trim()
prod #Displayes the product name
amazon_scraper <- function(doc, reviewer = T, delay = 0){
if(!"pacman" %in% installed.packages()[,"Package"]) install.packages("pacman")
pacman::p_load_gh("trinker/sentimentr")
pacman::p_load(RCurl, XML, dplyr, stringr, rvest, audio)
sec = 0
if(delay < 0) warning("delay was less than 0: set to 0")
if(delay > 0) sec = max(0, delay + runif(1, -1, 1))
#Remove all white space
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
title <- doc %>%
html_nodes("#cm_cr-review_list .a-color-base") %>%
html_text()
author <- doc %>%
html_nodes(".review-byline .author") %>%
html_text()
date <- doc %>%
html_nodes("#cm_cr-review_list .review-date") %>%
html_text() %>%
gsub(".*on ", "", .)
ver.purchase <- doc%>%
html_nodes(".review-data.a-spacing-mini") %>%
html_text() %>%
grepl("Verified Purchase", .) %>%
as.numeric()
format <- doc %>%
html_nodes(".review-data.a-spacing-mini") %>%
html_text() %>%
gsub("Color: |\\|.*|Verified.*", "", .)
#if(length(format) == 0) format <- NA
stars <- doc %>%
html_nodes("#cm_cr-review_list  .review-rating") %>%
html_text() %>%
str_extract("\\d") %>%
as.numeric()
comments <- doc %>%
html_nodes("#cm_cr-review_list .review-text") %>%
html_text()
helpful <- doc %>%
html_nodes(".cr-vote-buttons .a-color-secondary") %>%
html_text() %>%
str_extract("[:digit:]+|One") %>%
gsub("One", "1", .) %>%
as.numeric()
if(reviewer == T){
rver_url <- doc %>%
html_nodes(".review-byline .author") %>%
html_attr("href") %>%
gsub("/ref=cm_cr_othr_d_pdp\\?ie=UTF8", "", .) %>%
gsub("/gp/pdp/profile/", "", .) %>%
paste0("https://www.amazon.com/gp/cdp/member-reviews/",.)
#average rating of past 10 reviews
rver_avgrating_10 <- rver_url %>%
sapply(., function(x) {
read_html(x) %>%
html_nodes(".small span img") %>%
html_attr("title") %>%
gsub("out of.*|stars", "", .) %>%
as.numeric() %>%
mean(na.rm = T)
}) %>% as.numeric()
rver_prof <- rver_url %>%
sapply(., function(x)
read_html(x) %>%
html_nodes("div.small, td td td .tiny") %>%
html_text()
)
rver_numrev <- rver_prof %>%
lapply(., function(x)
gsub("\n  Customer Reviews: |\n", "", x[1])
) %>% as.numeric()
rver_numhelpful <- rver_prof %>%
lapply(., function(x)
gsub(".*Helpful Votes:|\n", "", x[2]) %>%
trim()
) %>% as.numeric()
rver_rank <- rver_prof %>%
lapply(., function(x)
gsub(".*Top Reviewer Ranking:|Helpful Votes:.*|\n", "", x[2]) %>%
removePunctuation() %>%
trim()
) %>% as.numeric()
df <- data.frame(title, date, ver.purchase, format, stars, comments, helpful,
rver_url, rver_avgrating_10, rver_numrev, rver_numhelpful, rver_rank, stringsAsFactors = F)
} else df <- data.frame(title, author, date, ver.purchase, format, stars, comments, helpful, stringsAsFactors = F)
return(df)
}
pages <- 10
reviews_all <- NULL
for(page_num in 1:pages){
url <- paste0("http://www.amazon.com/product-reviews/",prod_code,"/?pageNumber=", page_num)
doc <- read_html(url)
reviews <- amazon_scraper(doc, reviewer = F, delay = 2)
reviews_all <- rbind(reviews_all, cbind(prod, reviews))
}
install.package("trinker")
library(sentimentr) #Calculates text polarity sentiment at the sentence level
sent_agg <- with(reviews_all, sentiment_by(comments))
head(sent_agg)
par(mfrow=c(1,2))
with(reviews_all, hist(stars))
with(sent_agg, hist(ave_sentiment))
mean(reviews_all$stars)
mean(sent_agg$ave_sentiment) #Measured on a scale from -4 to 4 (where 0 is neutral)
#'Highlight' function in Sentimentr package helps us obtain the reviews with highest sentiment and lowest sentiment
best_reviews <- slice(reviews_all, top_n(sent_agg, 3, ave_sentiment)$element_id)
with(best_reviews, sentiment_by(comments)) %>% highlight() #Result is displayed in HTML
worst_reviews <- slice(reviews_all, top_n(sent_agg, 3, -ave_sentiment)$element_id)
with(worst_reviews, sentiment_by(comments)) %>% highlight() #Result is displayed in HTML
install.packages("tidyr")
install.packages("purrr")
library(tidyverse)
library(tidytext)
install.packages("purrr")
install.packages("tidyr")
install.packages("tidyr")
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative")) %>%
best_reviews %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words = 100))
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative")) %>%
reviews_all %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words = 100))
reviews %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(max.words = 150)
reviews_all %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(max.words = 150)
reviews_all %>%
cross_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(max.words = 150)
reviews_all %>%
full_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(max.words = 150)
reviews_all %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(max.words = 150)
prod_code = "B01DFKC2SO" #ASIN Code of a product
url <- paste0("https://www.amazon.com/dp/", prod_code) #URL of Amazon's product
doc <- read_html(url)
prod <- html_nodes(doc, "#productTitle") %>% html_text() %>% gsub("\n", "", .) %>% trim()
prod #Displayes the product name
amazon_scraper <- function(doc, reviewer = T, delay = 0){
if(!"pacman" %in% installed.packages()[,"Package"]) install.packages("pacman")
pacman::p_load_gh("trinker/sentimentr")
pacman::p_load(RCurl, XML, dplyr, stringr, rvest, audio)
sec = 0
if(delay < 0) warning("delay was less than 0: set to 0")
if(delay > 0) sec = max(0, delay + runif(1, -1, 1))
#Remove all white space
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
title <- doc %>%
html_nodes("#cm_cr-review_list .a-color-base") %>%
html_text()
author <- doc %>%
html_nodes(".review-byline .author") %>%
html_text()
date <- doc %>%
html_nodes("#cm_cr-review_list .review-date") %>%
html_text() %>%
gsub(".*on ", "", .)
ver.purchase <- doc%>%
html_nodes(".review-data.a-spacing-mini") %>%
html_text() %>%
grepl("Verified Purchase", .) %>%
as.numeric()
format <- doc %>%
html_nodes(".review-data.a-spacing-mini") %>%
html_text() %>%
gsub("Color: |\\|.*|Verified.*", "", .)
#if(length(format) == 0) format <- NA
stars <- doc %>%
html_nodes("#cm_cr-review_list  .review-rating") %>%
html_text() %>%
str_extract("\\d") %>%
as.numeric()
comments <- doc %>%
html_nodes("#cm_cr-review_list .review-text") %>%
html_text()
helpful <- doc %>%
html_nodes(".cr-vote-buttons .a-color-secondary") %>%
html_text() %>%
str_extract("[:digit:]+|One") %>%
gsub("One", "1", .) %>%
as.numeric()
if(reviewer == T){
rver_url <- doc %>%
html_nodes(".review-byline .author") %>%
html_attr("href") %>%
gsub("/ref=cm_cr_othr_d_pdp\\?ie=UTF8", "", .) %>%
gsub("/gp/pdp/profile/", "", .) %>%
paste0("https://www.amazon.com/gp/cdp/member-reviews/",.)
#average rating of past 10 reviews
rver_avgrating_10 <- rver_url %>%
sapply(., function(x) {
read_html(x) %>%
html_nodes(".small span img") %>%
html_attr("title") %>%
gsub("out of.*|stars", "", .) %>%
as.numeric() %>%
mean(na.rm = T)
}) %>% as.numeric()
rver_prof <- rver_url %>%
sapply(., function(x)
read_html(x) %>%
html_nodes("div.small, td td td .tiny") %>%
html_text()
)
rver_numrev <- rver_prof %>%
lapply(., function(x)
gsub("\n  Customer Reviews: |\n", "", x[1])
) %>% as.numeric()
rver_numhelpful <- rver_prof %>%
lapply(., function(x)
gsub(".*Helpful Votes:|\n", "", x[2]) %>%
trim()
) %>% as.numeric()
rver_rank <- rver_prof %>%
lapply(., function(x)
gsub(".*Top Reviewer Ranking:|Helpful Votes:.*|\n", "", x[2]) %>%
removePunctuation() %>%
trim()
) %>% as.numeric()
df <- data.frame(title, date, ver.purchase, format, stars, comments, helpful,
rver_url, rver_avgrating_10, rver_numrev, rver_numhelpful, rver_rank, stringsAsFactors = F)
} else df <- data.frame(title, author, date, ver.purchase, format, stars, comments, helpful, stringsAsFactors = F)
return(df)
}
pages <- 10
reviews_all <- NULL
for(page_num in 1:pages){
url <- paste0("http://www.amazon.com/product-reviews/",prod_code,"/?pageNumber=", page_num)
doc <- read_html(url)
reviews <- amazon_scraper(doc, reviewer = F, delay = 2)
reviews_all <- rbind(reviews_all, cbind(prod, reviews))
}
install.package("trinker")
library(sentimentr) #Calculates text polarity sentiment at the sentence level
sent_agg <- with(reviews_all, sentiment_by(comments))
head(sent_agg)
par(mfrow=c(1,2))
with(reviews_all, hist(stars))
with(sent_agg, hist(ave_sentiment))
with(reviews_all, hist(star))
with(reviews_all, hist(stars))
with(sent_agg, hist(ave_sentiment))
mean(reviews_all$stars)
mean(sent_agg$ave_sentiment) #Measured on a scale from -4 to 4 (where 0 is neutral)
#'Highlight' function in Sentimentr package helps us obtain the reviews with highest sentiment and lowest sentiment
best_reviews <- slice(reviews_all, top_n(sent_agg, 3, ave_sentiment)$element_id)
with(best_reviews, sentiment_by(comments)) %>% highlight() #Result is displayed in HTML
worst_reviews <- slice(reviews_all, top_n(sent_agg, 3, -ave_sentiment)$element_id)
with(worst_reviews, sentiment_by(comments)) %>% highlight() #Result is displayed in HTML
library(tidyverse)
library(tidytext)
word_tb <- reviews_all %>%
unnest_tokens(word, TXT)
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative")) %>%
reviews_all %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word,n,max.words = 100))
reviews_all %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(max.words = 150)
reviews_all %>%
group_by(stars) %>%
mutate(word_count = 1:n(),
index = word_count %/% 50 + 1) %>%
inner_join(get_sentiments("bing")) %>%
count(NAME, index = index , sentiment) %>%
ungroup() %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
ggplot(aes(index, sentiment, fill = sentiment > 0)) +
geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
facet_wrap(~ NAME, ncol = 2, scales = "free_x")
reviews_all %>%
group_by(stars) %>%
mutate(word_count = 1:n(),
index = word_count %/% 50 + 1) %>%
cross_join(get_sentiments("bing")) %>%
count(NAME, index = index , sentiment) %>%
ungroup() %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
ggplot(aes(index, sentiment, fill = sentiment > 0)) +
geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
facet_wrap(~ NAME, ncol = 2, scales = "free_x")
reviews_all %>%
group_by(stars) %>%
mutate(word_count = 1:n(),
index = word_count %/% 50 + 1) %>%
full_join(get_sentiments("bing")) %>%
count(NAME, index = index , sentiment) %>%
ungroup() %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
ggplot(aes(index, sentiment, fill = sentiment > 0)) +
geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
facet_wrap(~ NAME, ncol = 2, scales = "free_x")
reviews_all %>%
group_by(stars) %>%
mutate(word_count = 1:n(),
index = word_count %/% 50 + 1) %>%
outer_join(get_sentiments("bing")) %>%
count(NAME, index = index , sentiment) %>%
ungroup() %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
ggplot(aes(index, sentiment, fill = sentiment > 0)) +
geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
facet_wrap(~ NAME, ncol = 2, scales = "free_x")
best_reviews <- slice(reviews_all, top_n(sent_agg, 10, ave_sentiment)$element_id)
with(best_reviews, sentiment_by(comments)) %>% highlight() #Result is displayed in HTML
worst_reviews <- slice(reviews_all, top_n(sent_agg, 10, -ave_sentiment)$element_id)
with(worst_reviews, sentiment_by(comments)) %>% highlight() #Result is displayed in HTML
install.packages("rvest")
install.packages("rvest")
library(rvest)
review=data.frame(ReviewContent=character(),stringsAsFactors=FALSE)
for(i in 1:500)#Pagenation through 500 pages for 5000 reviews
{
url=paste("https://www.amazon.com/Amazon-Kindle-Paperwhite-6-Inch-4GB-eReader/product-reviews/B00OQVZDJM/ref=cm_cr_arp_d_paging_btm_",i,"?ie=UTF8&reviewerType=all_reviews&pageNumber=",i,sep="")
review_data <- read_html(url)
text <- review_data %>%
html_nodes("#cm_cr-review_list .review .review-data .review-text") %>%
html_text()
for(j in 1:length(text))
{
review[nrow(review)+1,] <- c(text[j])
}
}
write.csv(review,file = "desktop/AmazonReviews.csv",row.names = FALSE)
library(tm)
library(NLP)
install.packages("qdap")
library(qdap)
library(stringr)
positive_lexicons <- read.csv(file="positive-words.csv", header=TRUE, sep= ',')
positive_lexicons <- Corpus(VectorSource(positive_lexicons)) #Converting to corpus
positive_lexicons <- tm_map(positive_lexicons,stemDocument) #Stemming
positive_lexicons <- data.frame(text=sapply(positive_lexicons, `[[`, "content"), stringsAsFactors=FALSE) #Converting to dataframe
positive_lexicons <- unique(positive_lexicons) #Fetching only unique positive words
positive_lexicons<-data.frame(Words=unlist(positive_lexicons)) #Unlisting
negative_lexicons <- read.csv(file="negative-words.csv", header=TRUE, sep= ',')
negative_lexicons <- Corpus(VectorSource(negative_lexicons)) #Converting to corpus
negative_lexicons <- tm_map(negative_lexicons,stemDocument) #Stemming
negative_lexicons <- data.frame(text=sapply(negative_lexicons, `[[`, "content"), stringsAsFactors=FALSE) #Converting to dataframe
negative_lexicons <- unique(negative_lexicons) #Fetching only unique positive words
negative_lexicons<-data.frame(Words=unlist(negative_lexicons)) #Unlisting
df <- data.frame("Review"=character(),"Positive Word Count"=integer(),"Negative Word Count"=integer(),"Total Word Count"=integer(),"Positivity Percentage"=integer(),"Negativity Percentage"=integer(),"Result"=character(),stringsAsFactors = FALSE)
review <- read.csv(file="Reviews.csv", header=TRUE, sep= ',')
NROW(review) #Number of rows of dataframe 'review'
for(k in 1:NROW(review))
{
review_data=review[k,]  #To extract data from each row and all columns
review_original<-review_data
df[nrow(df)+1,1]<-c(toString(review_original))
review_data = tolower(review_data) #Making it lower case
review_data = gsub('[[:punct:]]', '', review_data) #Removing  punctuation
review_data = gsub("[[:digit:]]", "", review_data) #Removing numbers
review_data <- Corpus(VectorSource(review_data)) #Converting into corpus
review_data = tm_map(review_data, removeWords, stopwords('english'))  #Removing stop words
review_data=tm_map(review_data,stemDocument) #Stemming
#strwrap(b[[1]]) #To view the stemmed data
review_data <- data.frame(text=sapply(review_data, `[[`, "content"), stringsAsFactors=FALSE)#Converting corpus to dataframe
#review_data
#typeof(review_data)
review_data<-str_trim(clean(review_data)) #To remove extra white spaces
review_data<- as.String(review_data)
review_words <- strsplit(review_data, " ") #Splitting a sentence into words
length(review_words)
review_words<-data.frame(Words=unlist(review_words)) #Unlisting
review_words<-as.matrix(review_words,nrow=NROW(review_words),ncol=NCOL(review_words)) #Matrix
NROW(review_words)
positive_count=0
negative_count=0
total_word_count=NROW(review_words)
for(i in 1:NROW(review_words)) #Each word of that review
{
if(review_words[i][1] %in% positive_lexicons$Words)
positive_count=positive_count+1
else if (review_words[i][1] %in% negative_lexicons$Words)
negative_count=negative_count+1
}
positive_count
negative_count
total_word_count
positivity_percentage=(positive_count/total_word_count)*100
negativity_percentage=(negative_count/total_word_count)*100
result=""
if(positivity_percentage>negativity_percentage)
{result='Positive'
}else
{result='Negative'}
result
df[nrow(df),2:7]<- c(positive_count,negative_count,total_word_count,positivity_percentage,negativity_percentage,result)
#df
}
write.csv(df,"Sentimental_Analysis_Result.csv")
install.packages("snowballC")
library(rvest)
review=data.frame(ReviewContent=character(),stringsAsFactors=FALSE)
for(i in 1:500) #Scraping through 500 webpages
{
url=paste("https://www.amazon.com/dp/B01DFKC2SO",i,sep="")
review_data <- read_html(url)
text <- review_data %>%
html_nodes("#cm_cr-review_list .review .review-data .review-text") %>%
html_text()
text #Displayes the product name
for(j in 1:length(text))
{
review[nrow(review)+1,] <- c(text[j])
}
}
write.csv(review,file = "desktop/AmazonReviews.csv",row.names = FALSE)
for(i in 1:500) #Scraping through 500 webpages
{
url=paste("https://www.amazon.com/dp/B01DFKC2SO",i,sep="")
review_data <- read_html(url)
text <- review_data %>%
html_nodes("#cm_cr-review_list .review .review-data .review-text") %>%
html_text()
#text #Displayes the product name
for(j in 1:length(text))
{
review[nrow(review)+1,] <- c(text[j])
}
}
